# Data Engineering ‚Äî –†–∞–∑–¥–µ–ª 4. HDFS –∏ Apache Spark

**–ê–≤—Ç–æ—Ä:** –©–µ–≥–æ–ª–µ–≤ –ú–∏—Ö–∞–∏–ª  
**–ö—É—Ä—Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –¥–ª—è –ª–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, 2025**

---

## üìò –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [HDFS ‚Äî –æ—Å–Ω–æ–≤—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö](#hdfs--–æ—Å–Ω–æ–≤—ã-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ-—Ö—Ä–∞–Ω–µ–Ω–∏—è-–¥–∞–Ω–Ω—ã—Ö)
2. [–í–≤–µ–¥–µ–Ω–∏–µ –≤ Apache Spark](#–≤–≤–µ–¥–µ–Ω–∏–µ-–≤-apache-spark)
3. [DataFrame API –∏ RDD](#dataframe-api-–∏-rdd)
4. [Transformations –∏ Actions](#transformations-–∏-actions)
5. [–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, broadcast –∏ shuffle](#–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ-broadcast-–∏-shuffle)
6. [–ú–∏–Ω–∏-–ø—Ä–æ–µ–∫—Ç: ETL-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ Spark](#–º–∏–Ω–∏–ø—Ä–æ–µ–∫—Ç-etl–ø–∞–π–ø–ª–∞–π–Ω-–Ω–∞-spark)
7. [–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏](#–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ-–≤–æ–ø—Ä–æ—Å—ã-–∏-gpt–ø–æ–¥—Å–∫–∞–∑–∫–∏)

---

## HDFS ‚Äî –æ—Å–Ω–æ–≤—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

üìò –ü—Ä–æ—á–∏—Ç–∞—Ç—å: *Index ‚Äî Roadmappers (—Ä–∞–∑–¥–µ–ª HDFS)*  

### üß† –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
- **HDFS (Hadoop Distributed File System)** ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Ñ–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö.  
- **NameNode** ‚Äî —Ö—Ä–∞–Ω–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Ñ–∞–π–ª–∞—Ö (–≥–¥–µ –∫–∞–∫–∏–µ –±–ª–æ–∫–∏ –ª–µ–∂–∞—Ç).  
- **DataNode** ‚Äî —Ö—Ä–∞–Ω–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö.  
- **Replication Factor** ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–ø–ª–∏–∫–∞—Ü–∏–∏ (–æ–±—ã—á–Ω–æ 3).  

üí° *–ü—Ä–∏–º–µ—Ä:* –µ—Å–ª–∏ —Ñ–∞–π–ª 600 –ú–ë –∏ —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ 128 –ú–ë, HDFS —Ä–∞–∑–æ–±—å—ë—Ç –µ–≥–æ –Ω–∞ 5 –±–ª–æ–∫–æ–≤ –∏ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É–∑–ª–∞—Ö.

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –û–±—ä—è—Å–Ω–∏, –∫–∞–∫ HDFS –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å.  
> –ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥—ë—Ç, –µ—Å–ª–∏ NameNode –≤—ã–π–¥–µ—Ç –∏–∑ —Å—Ç—Ä–æ—è?

---

## –í–≤–µ–¥–µ–Ω–∏–µ –≤ Apache Spark

üìò –ú–∞—Ç–µ—Ä–∏–∞–ª—ã: *Index ‚Äî Roadmappers (—Ä–∞–∑–¥–µ–ª Spark)*

### üß† –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Spark
- **Driver** ‚Äî –≥–ª–∞–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, —É–ø—Ä–∞–≤–ª—è—é—â–∏–π –∑–∞–¥–∞—á–∞–º–∏.  
- **Executor** ‚Äî –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ —Ä–∞–±–æ—á–µ–º —É–∑–ª–µ, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π –∑–∞–¥–∞—á–∏.  
- **SparkContext** ‚Äî –æ–±—ä–µ–∫—Ç, —É–ø—Ä–∞–≤–ª—è—é—â–∏–π –≤—Å–µ–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞.  
- **Cluster Manager** ‚Äî YARN / Kubernetes / Standalone.

### üíª –ü—Ä–∏–º–µ—Ä

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ExampleApp") \
    .master("local[*]") \
    .getOrCreate()

print(spark.version)
```

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –û–±—ä—è—Å–Ω–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç SparkContext.  
> –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Spark job ‚Äî —à–∞–≥ –∑–∞ —à–∞–≥–æ–º?

---

## DataFrame API –∏ RDD

### üß† –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É RDD –∏ DataFrame
- **RDD (Resilient Distributed Dataset)** ‚Äî –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ API (–±–ª–∏–∂–µ –∫ MapReduce).  
- **DataFrame API** ‚Äî –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å, –ø–æ—Ö–æ–∂ –Ω–∞ SQL-—Ç–∞–±–ª–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Catalyst Optimizer.  

üí° *–°–æ–≤–µ—Ç:* –í 90% —Å–ª—É—á–∞–µ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç DataFrame API.

### üíª –ü—Ä–∏–º–µ—Ä

```python
from pyspark.sql import functions as F

df = spark.read.csv("data/sales.csv", header=True, inferSchema=True)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
sales_by_region = (
    df.groupBy("region")
      .agg(F.sum("amount").alias("total_sales"))
      .orderBy(F.desc("total_sales"))
)

sales_by_region.show()
```

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –û–±—ä—è—Å–Ω–∏, –∫–∞–∫ Catalyst Optimizer —É—Å–∫–æ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã.  
> –ü—Ä–∏–¥—É–º–∞–π –∑–∞–¥–∞—á—É, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RDD –≤—Å—ë –µ—â—ë –æ–ø—Ä–∞–≤–¥–∞–Ω–æ.

---

## Transformations –∏ Actions

### üß† –¢–µ–æ—Ä–∏—è
- **Transformations** ‚Äî –æ–ø–µ—Ä–∞—Ü–∏–∏, —Å–æ–∑–¥–∞—é—â–∏–µ –Ω–æ–≤—ã–π DataFrame (`select`, `filter`, `groupBy`, `join`).  
- **Actions** ‚Äî –æ–ø–µ—Ä–∞—Ü–∏–∏, –∑–∞–ø—É—Å–∫–∞—é—â–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (`show`, `collect`, `count`).  

Spark –ø—Ä–∏–º–µ–Ω—è–µ—Ç *–ª–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è* (lazy evaluation): –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –ø–æ–∫–∞ –Ω–µ –≤—ã–∑–≤–∞–Ω–∞ action.

### üíª –ü—Ä–∏–º–µ—Ä

```python
from pyspark.sql import functions as F

df = spark.read.csv("data/users.csv", header=True, inferSchema=True)

# –≠—Ç–æ transformation ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π DataFrame, –Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç
filtered = df.filter(F.col("age") > 25)

# –≠—Ç–æ action ‚Äî Spark —Ä–µ–∞–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—á—ë—Ç—ã
filtered.show()
```

üí° *–°–æ–≤–µ—Ç:* –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–∑–±–µ–≥–∞–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö `collect()` ‚Äî –æ–Ω–∏ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç –¥–∞–Ω–Ω—ã–µ –≤ –¥—Ä–∞–π–≤–µ—Ä.

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –ü—Ä–∏–¥—É–º–∞–π –ø—Ä–∏–º–µ—Ä, –≥–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ transformations –∏ –æ–¥–Ω–æ action.  
> –û–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å lazy evaluation.

---

## –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, broadcast –∏ shuffle

### üß† –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
- **cache() / persist()** ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—é—Ç DataFrame –≤ –ø–∞–º—è—Ç–∏, —á—Ç–æ–±—ã –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ –ø–µ—Ä–µ—Å—á—ë—Ç–∞.  
- **broadcast()** ‚Äî –ø–µ—Ä–µ–¥–∞—ë—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –≤—Å–µ —É–∑–ª—ã, —É—Å–∫–æ—Ä—è—è join.  
- **shuffle** ‚Äî –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —É–∑–ª–∞–º–∏ –ø—Ä–∏ `groupBy`, `join`, `repartition`.

### üíª –ü—Ä–∏–º–µ—Ä

```python
# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
users = spark.read.parquet("data/users.parquet").cache()

# Broadcast join
small_df = spark.read.csv("data/countries.csv", header=True)
broadcast_df = F.broadcast(small_df)

joined = users.join(broadcast_df, "country", "left")
joined.show()
```

üí° *–°–æ–≤–µ—Ç:* –∏–∑–±–µ–≥–∞–π shuffle, –∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å broadcast.

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ shuffle –∏ –∫–∞–∫ –µ–≥–æ –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å?  
> –ö–æ–≥–¥–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å persist() –≤–º–µ—Å—Ç–æ cache()?

---

## –ú–∏–Ω–∏-–ø—Ä–æ–µ–∫—Ç: ETL-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ Spark

### üéØ –¶–µ–ª—å
–°–æ–∑–¥–∞—Ç—å Spark-–ø–∞–π–ø–ª–∞–π–Ω, –∫–æ—Ç–æ—Ä—ã–π:
1. –°—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV –≤ HDFS.  
2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, –∞–≥—Ä–µ–≥–∞—Ü–∏—è).  
3. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ –≤ HDFS –≤ —Ñ–æ—Ä–º–∞—Ç–µ Parquet.

### üíª –ü—Ä–∏–º–µ—Ä

```python
from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.appName("ETL_Pipeline").getOrCreate()

# 1. –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS
df = spark.read.csv("hdfs:///data/sales.csv", header=True, inferSchema=True)

# 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
agg_df = (
    df.filter(F.col("amount") > 100)
      .groupBy("region")
      .agg(F.sum("amount").alias("total"))
)

# 3. –ó–∞–ø–∏—Å—å –æ–±—Ä–∞—Ç–Ω–æ –≤ HDFS
agg_df.write.mode("overwrite").parquet("hdfs:///data/output/sales_summary")

spark.stop()
```

üí° *–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:* –¥–æ–±–∞–≤—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–æ–≤ –≤ Airflow DAG (–≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö).

### üí° GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏
> –ü–æ–ø—Ä–æ—Å–∏ GPT –¥–æ–±–∞–≤–∏—Ç—å –≤ —ç—Ç–æ—Ç –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.  
> –ü–æ–ø—Ä–æ—Å–∏ –ø—Ä–∏–º–µ—Ä DAG –≤ Airflow, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∑—ã–≤–∞–µ—Ç —ç—Ç–æ—Ç Spark job.

---

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ GPT-–ø–æ–¥—Å–∫–∞–∑–∫–∏

1. –ß—Ç–æ —Ç–∞–∫–æ–µ HDFS –∏ –∑–∞—á–µ–º –Ω—É–∂–µ–Ω NameNode?  
2. –ß–µ–º DataFrame –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç RDD?  
3. –ß—Ç–æ —Ç–∞–∫–æ–µ transformations –∏ actions –≤ Spark?  
4. –ß—Ç–æ —Ç–∞–∫–æ–µ lazy evaluation?  
5. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç broadcast join –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?  
6. –ß—Ç–æ —Ç–∞–∫–æ–µ shuffle –∏ –∫–∞–∫ –µ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å?  
7. –ß–µ–º cache –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç persist?

üí° *–°–æ–≤–µ—Ç:* –µ—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî —Å–ø—Ä–æ—Å–∏ GPT:
> ¬´–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ Spark –≤—ã–ø–æ–ª–Ω—è–µ—Ç join –ø–æ—à–∞–≥–æ–≤–æ¬ª  
> ¬´–ü–æ—á–µ–º—É Spark –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º MapReduce?¬ª

---

‚úÖ **–ò—Ç–æ–≥ —Ä–∞–∑–¥–µ–ª–∞:**  
- –ü–æ–Ω–∏–º–∞–µ—à—å –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã HDFS –∏ Spark.  
- –£–º–µ–µ—à—å –ø–∏—Å–∞—Ç—å PySpark-–∫–æ–¥ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.  
- –ì–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Spark –≤ Airflow-–ø–∞–π–ø–ª–∞–π–Ω—ã.
